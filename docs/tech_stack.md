# **Finance QA Chatbot – Tech Stack Overview**

This document summarizes the tools and frameworks chosen for each part of the neural pipeline used in the Finance QA Chatbot project.


## **1. Data Layer – DuckDB**

We use **DuckDB** as the main analytical database.
It stores the trial balance data and executes SQL queries generated by the model.
DuckDB is lightweight, fast for local analytics, and integrates well with Python and Pandas.


## **2. Embeddings – MiniLM**

We use **MiniLM (all-MiniLM-L6-v2)** from the Sentence Transformers library to create vector embeddings of financial account names and descriptions.
These embeddings help the system understand semantically similar terms (for example, “sales income” and “revenue”).


## **3. Vector Search – FAISS or Chroma**

To find the most relevant financial accounts, we store embeddings in a **vector database** using **FAISS** or **Chroma**.
Both support efficient similarity search. We’ll start with Chroma because it’s easy to integrate, and later switch to FAISS if performance becomes a concern.


## **4. Text-to-SQL – BERT + PICARD**

The chatbot uses a **BERT- or T5-based text-to-SQL model** to translate user questions into SQL queries.
We add **PICARD** to validate generated SQL syntax as it’s produced.
This step connects natural language directly to the database, making questions like “What was our revenue in 2023?” executable.


## **5. Answer Generation – GPT-4 or LLaMA 3**

Once the SQL query runs, the result (a number or table) is sent to an **LLM** such as **GPT-4** or **LLaMA 3**.
The model reformats raw outputs into natural sentences like:

> “Total revenue in 2023 was 10.3 million, up 5% from the previous year.”

This gives the system a clear and human-readable way to communicate answers.


## **6. Backend – FastAPI**

The backend uses **FastAPI** to connect all pipeline parts into a single endpoint `/ask`.
It handles input normalization, runs the model pipeline, and returns the answer as JSON.
FastAPI is modern, lightweight, and works well with async Python tasks.


## **7. Frontend – Streamlit**

We use **Streamlit** to build a simple web interface.
It lets users type questions, see responses, and visualize numeric trends through small charts.
This keeps the focus on the functionality without heavy frontend work.


## **8. Evaluation & Logging**

We’ll log each query, generated SQL, runtime, and result to a **CSV or SQLite** file.
This makes it easy to evaluate accuracy and track improvements over time.


## **Summary Table**

| Layer             | Purpose                              | Tool                |
| ----------------- | ------------------------------------ | ------------------- |
| Data Storage      | Store and query financial data       | **DuckDB**          |
| Embeddings        | Vectorize account names/descriptions | **MiniLM**          |
| Vector Search     | Find similar accounts                | **FAISS / Chroma**  |
| Text-to-SQL       | Generate valid SQL queries           | **BERT + PICARD**   |
| Answer Generation | Write natural language responses     | **GPT-4 / LLaMA 3** |
| Backend           | Orchestrate pipeline                 | **FastAPI**         |
| Frontend          | Chat interface                       | **Streamlit**       |
| Evaluation        | Logging and performance tracking     | **CSV / SQLite**    |


**Team allocation**

* **Xander:** models, text-to-SQL, and tuning
* **Anh:** data and embeddings
* **Fion:** API and backend integration
* **Joshia:** UI and evaluation
